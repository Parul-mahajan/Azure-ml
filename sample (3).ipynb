{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49e8c91-1527-4dd6-a2cf-6bdb34281bc7",
   "metadata": {},
   "source": [
    "# Importing Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4898ec31-5924-45dd-8a7a-d32ced33acaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.56.0\n",
      "azureml_demo loaded\n",
      "Ready to use Azure ML 1.56.0 to work with azureml_demo\n"
     ]
    }
   ],
   "source": [
    "# %pip install azureml-core\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "auth = InteractiveLoginAuthentication()\n",
    "from operator import itemgetter\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Datastore\n",
    "from azureml.core import Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import Model\n",
    "from azureml.core import Environment, Experiment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.compute import AksCompute\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "print(\"Ready to use Azure ML\", azureml.core.VERSION)\n",
    "\n",
    "\n",
    "config_file_path = './config.json'\n",
    "error_type_message = 'Error type:'\n",
    "json_loads_error_message = 'json.loads() ValueError for JSON object:'\n",
    "\n",
    "# This setup_config.json file establish session based on custom config\n",
    "try:\n",
    "    with open(config_file_path) as setup_config_data:\n",
    "        setup_config = json.load(setup_config_data)\n",
    "except ValueError as error:\n",
    "    print(f\"{error_type_message} {type(error)}\")\n",
    "    print(f\"{json_loads_error_message} {error}\")\n",
    "\n",
    "subscription_id = setup_config['subscription_id']\n",
    "resource_group = setup_config['resource_group']\n",
    "workspace_name = setup_config['workspace_name']\n",
    "\n",
    "ws = Workspace(\n",
    "    subscription_id=subscription_id,\n",
    "    resource_group=resource_group,\n",
    "    workspace_name=workspace_name\n",
    ")\n",
    "\n",
    "print(ws.name, \"loaded\")\n",
    "keyvault = ws.get_default_keyvault()\n",
    "# ws.get_details()\n",
    "print(\n",
    "    'Ready to use Azure ML {} to work with {}'.format(\n",
    "        azureml.core.VERSION,\n",
    "        ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01a245c-3754-49dc-8523-0768d5ee2a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pip_MSAI\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder for the pipeline step files\n",
    "experiment_folder = 'test_pip_MSAI'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf6fb05-d7fd-480c-a3dd-22fde07fb67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_pip_MSAI/Training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/Training.py\n",
    "from azure.identity import ManagedIdentityCredential, DefaultAzureCredential\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from azureml.core import Run\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pdfplumber\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import chardet\n",
    "from decimal import Decimal, InvalidOperation\n",
    "import glob\n",
    "import azure.ai.formrecognizer\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "np.int = np.int32\n",
    "np.float = np.float64\n",
    "np.bool = np.bool_\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import json\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import fitz\n",
    "# import PyMuPDF\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import ast\n",
    "import multiprocessing\n",
    "import os\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "import langdetect\n",
    "from langdetect import detect\n",
    "import ast\n",
    "\n",
    "\n",
    "LOCAL_RUN = 'N'\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "if LOCAL_RUN == 'N':\n",
    "    # Get the experiment run contexty\n",
    "    run = Run.get_context()\n",
    "    ws = run.experiment.workspace\n",
    "else:\n",
    "    # Load the workspace from the saved config file\n",
    "    ws = Workspace.from_config()\n",
    "\n",
    "\n",
    "print('Ready to work with {}'.format(ws.name))\n",
    "\n",
    "keyvault = ws.get_default_keyvault()\n",
    "\n",
    "identity_client_id = keyvault.get_secret(\"clientid\")\n",
    "\n",
    "credential = ManagedIdentityCredential(client_id=identity_client_id)\n",
    "print(\"credential\", credential)\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = \"https://openai-finops-np-002.openai.azure.com/\"\n",
    "openai.api_version = \"2024-02-15-preview\"\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    " \n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = openai.api_base, \n",
    "  azure_ad_token_provider=token_provider, \n",
    "  api_version=openai.api_version\n",
    ") \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def get_completion(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"openainpfinos02\",\n",
    "        seed=42,\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
    "        {\"role\": \"user\", \"content\":prompt}])\n",
    "    response = response.choices[0].message.content\n",
    "    return response  \n",
    "\n",
    "\n",
    "def form_reading_pdf(filename, endpoint, credential):\n",
    "    print(\"inside form recognizer\")\n",
    "    client = DocumentAnalysisClient(endpoint, credential = credential)\n",
    " \n",
    "    # Use the custom model to extract information from the document\n",
    "    with open(filename, \"rb\") as f:\n",
    "        poller = client.begin_analyze_document(\"prebuilt-layout\", f.read())\n",
    "        result = poller.result()\n",
    " \n",
    "    output = {}\n",
    "    page_number = 1  # Initialize page number\n",
    "    for page in result.pages:\n",
    "        text = \"\"\n",
    "        for line in page.lines:\n",
    "            text += line.content + \"\\n\"\n",
    "        output['Page number ' + str(page_number)] = text.strip()\n",
    "        page_number += 1  # Increment page number\n",
    " \n",
    "    return json.dumps(output)\n",
    "\n",
    "\n",
    "def form_convert_dict_to_list_of_dicts(input_string):\n",
    "    input_dict = ast.literal_eval(input_string)\n",
    "    list_of_dicts = []\n",
    "    for key, value in input_dict.items():\n",
    "        list_of_dicts.append({key: value})\n",
    "    return list_of_dicts\n",
    "\n",
    "def convert_dict_to_list_of_dicts(input_dict):\n",
    "    # input_dict = ast.literal_eval(input_string)\n",
    "    list_of_dicts = []\n",
    "    for key, value in input_dict.items():\n",
    "        list_of_dicts.append({key: value})\n",
    "    return list_of_dicts\n",
    "\n",
    "def reading_pdf(filename, endpoint, credential):\n",
    "    print(\"inside reading pdf func\",filename)\n",
    "    doc = fitz.open(filename)\n",
    "    page = doc[0]\n",
    "    if len(page.get_text())>100:\n",
    "        print(\"********pdf is editable*********\",len(page.get_text()))\n",
    "        \n",
    "    # print(len(doc))\n",
    "        final_dict = {}\n",
    "        for i in range(len(doc)):\n",
    "            page = doc[i]  # The third page is at index 2 \n",
    "            page_content = page.get_text()\n",
    "            page_number_key = f\"Page number {i+1}\"\n",
    "            final_dict[page_number_key] = page_content\n",
    "            final_dict_1 = convert_dict_to_list_of_dicts(final_dict)\n",
    "        return final_dict_1\n",
    "\n",
    "    else:\n",
    "        print(\"**************pdf is not editable*************\")\n",
    "        final_dict = form_reading_pdf(filename, endpoint, credential)\n",
    "        final_dict_1 = form_convert_dict_to_list_of_dicts(final_dict)\n",
    "        return final_dict_1\n",
    "    \n",
    "model1 = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "## Compute embeddings\n",
    "def get_embeddings(text, output_path_emb):\n",
    "    print(\"inside embeddings\")\n",
    "    embeddings = model1.encode(text)\n",
    "    with open(output_path_emb, 'wb') as f:\n",
    "        pickle.dump(embeddings, f)\n",
    "    # return embeddings\n",
    "    \n",
    "def load_embeddings_from_pickle(embeddings_path):\n",
    "    print(\"Inside load embedding func\")\n",
    "    with open(embeddings_path, 'rb') as f:\n",
    "        embeddings = pickle.load(f)\n",
    "    return embeddings\n",
    "\n",
    "def save_list_to_file(pdf_text, output_path):\n",
    "    print(\"inside save_list_to_file\")\n",
    "    pdf_text = str(pdf_text)\n",
    "    with open(output_path, \"w\") as file:\n",
    "        file.write(pdf_text)\n",
    "        \n",
    "def read_list_from_file(filename):\n",
    "    print(\"inside read_list_from_file\")\n",
    "    with open(filename, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "def find_closely_related_chunks(query, embeddings, text, threshold=0.0, top_k=15):\n",
    "    similarity_scores = cosine_similarity([query], embeddings)[0]\n",
    "    above_threshold_indices = np.where(similarity_scores > threshold)[0]\n",
    "    top_k_indices = above_threshold_indices[np.argsort(similarity_scores[above_threshold_indices])[::-1][:top_k]]\n",
    "    \n",
    "    print(\"Top 3 similarity scores:\",format(top_k))\n",
    "    for idx in top_k_indices:\n",
    "        print(\"Index {}: {}\".format(idx, similarity_scores[idx]))\n",
    "    \n",
    "    closely_related_chunks = []\n",
    "    for idx in top_k_indices:\n",
    "        chunk_key = list(text[idx].keys())[0]\n",
    "        similarity_score = similarity_scores[idx]\n",
    "        closely_related_chunks.append({f\"{chunk_key}, similarity_score: {similarity_score:.8f}\": text[idx][chunk_key]})\n",
    "    \n",
    "    return closely_related_chunks\n",
    "\n",
    "def create_chunks(list_of_dicts,chunk_size=1):\n",
    "    # print(\"list_of_dicts-------------\",len(list_of_dicts))\n",
    "    # print(\"list_of_dicts-------------\",list_of_dicts)\n",
    "    chunks = []\n",
    "    for i in range(0, len(list_of_dicts), chunk_size):\n",
    "        chunk = list_of_dicts[i:i+chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def summary1(contract):\n",
    "    prompt = f\"\"\"\n",
    "    You will be provided a contract details {contract}. You need to compose a brief description of the contract, specifying the parties involved as the purchaser, the supplier, and affliates and clarify their roles. Display the response in JSON format.\"\"\"\n",
    "    response = get_completion(prompt)\n",
    "    # print('response---------------------------------------',response)\n",
    "    return response\n",
    " \n",
    "def extract_text_from_image(image_bytes, endpoint, credential):\n",
    "    # Initialize DocumentAnalysisClient\n",
    "    client = DocumentAnalysisClient(endpoint, credential = credential)\n",
    " \n",
    "    # Call Azure Form Recognizer to extract text from the image\n",
    "    poller = client.begin_analyze_document(\"prebuilt-layout\", image_bytes)\n",
    "    result = poller.result()\n",
    " \n",
    "    # Extract text from the result\n",
    "    text = \"\"\n",
    "    for page in result.pages:\n",
    "        for line in page.lines:\n",
    "            text += line.content + \"\\n\"\n",
    " \n",
    "    return text.strip()\n",
    " \n",
    "def extract_text_from_pdf(pdf_path, endpoint, credential):\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    output = []\n",
    "    output1 = []\n",
    "    # Initialize DocumentAnalysisClient\n",
    "    client = DocumentAnalysisClient(endpoint, credential = credential)\n",
    " \n",
    "    # Open the PDF file\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            image_list = page.get_images(full=True)\n",
    "\n",
    "            if image_list:\n",
    "                print('image')\n",
    "                # Extract text from the image using Azure Form Recognizer\n",
    "                image_bytes = page.get_pixmap().tobytes()\n",
    "                extracted_text = extract_text_from_image(image_bytes, endpoint, credential)\n",
    "                output1.append({f\"Page number {page_num+1}\": extracted_text})\n",
    "            else:\n",
    "                text = page.get_text()            \n",
    "                # print('editable')\n",
    "                output.append({f\"Page number {page_num+1}\": text.strip()})\n",
    " \n",
    "    return output1\n",
    "\n",
    "def format_response(text, response):\n",
    "    \n",
    "    # Step 1: Extract clause reference value\n",
    "    clause_reference = next((value for key, value in response.items() if 'clause' in key.lower()), None)\n",
    "\n",
    "    # Step 2: Automatically remove keys containing the word \"clause\"\n",
    "    keys_to_remove = [key for key in response if 'clause' in key.lower()]\n",
    "    for key in keys_to_remove:\n",
    "        response.pop(key, None)\n",
    "    # print('updated response---------------',response)\n",
    "\n",
    "    # Step 3: Extract page number and similarity score\n",
    "    page_number = \"\"\n",
    "    similarity_score = \"\"\n",
    "    for item in text:\n",
    "        for key in item.keys():\n",
    "            match = re.search(r'Page number (\\d+), similarity_score: (\\d+\\.\\d+)', key)\n",
    "            if match:\n",
    "                page_number = match.group(1)\n",
    "                similarity_score = float(match.group(2))\n",
    "\n",
    "    # Step 4: Determine rank\n",
    "    if similarity_score > 0.3:\n",
    "        rank = \"Rank High\"\n",
    "    elif 0.2 <= similarity_score <= 0.3:\n",
    "        rank = \"Rank Medium\"\n",
    "    else:\n",
    "        rank = \"Rank Low\"\n",
    "\n",
    "    # Step 4: Update response dictionary\n",
    "    for key in response:\n",
    "        response[key] = f\"{response[key]}:{clause_reference}:{rank}:Page {page_number}:score {similarity_score:.2f}\"\n",
    "    \n",
    "    return response\n",
    "    # print('new response------------------------------',response)\n",
    "    \n",
    "    \n",
    "def map_keys_by_order(data, new_keys):\n",
    "    if len(data) != len(new_keys):\n",
    "        raise ValueError(\"Number of new keys must match the number of keys in the dictionary.\")\n",
    "\n",
    "    updated_dict = {}\n",
    "    for i, (key, value) in enumerate(data.items()):\n",
    "        updated_dict[new_keys[i]] = value\n",
    "\n",
    "    return updated_dict\n",
    "    \n",
    "def is_numeric(val):\n",
    "    try:\n",
    "        int(val)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def process_payment_terms(data):\n",
    "    payment_terms = data.get('PaymentTerms', '').lower()\n",
    "    payment_terms_in_days = data.get('PaymentTermsInDays', '')\n",
    "\n",
    "    if payment_terms == 'numeric':\n",
    "        if is_numeric(payment_terms_in_days):\n",
    "            return data\n",
    "        else:\n",
    "            for key in data:\n",
    "                data[key] = 'na'\n",
    "            return data\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "    \n",
    "def process_payment_clause(data):\n",
    "    payment_terms_ref = data.get('Payment Terms_Clause Reference', '').lower()\n",
    "    payment_terms = data.get('PaymentTerms', '').lower()\n",
    "    payment_terms_in_days = data.get('PaymentTermsInDays', '').lower()\n",
    "\n",
    "    if payment_terms_ref != 'na' and payment_terms == 'na' and payment_terms_in_days == 'na':\n",
    "        for key in data:\n",
    "            data[key] = 'na'\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_dict(d, keys, valid_values):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Converts dictionary values to uppercase and replaces them if they match valid_values.\n",
    " \n",
    "    Args:\n",
    "\n",
    "        d (dict): The dictionary containing the values to be processed.\n",
    "\n",
    "        valid_values (list): List of valid values.\n",
    " \n",
    "    Returns:\n",
    "\n",
    "        dict: The modified dictionary.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert valid_values to lowercase\n",
    "    lower_valid_values = [value.lower() for value in valid_values]\n",
    " \n",
    "    # Convert dictionary values to lowercase and replace if valid\n",
    "\n",
    "    for key, value in d.items():\n",
    "        if key == keys:\n",
    "            # print(keys)\n",
    "            if isinstance(value, str):\n",
    "                # print(value)\n",
    "                lower_value = value.lower()\n",
    "                # print(\"lower_value\", lower_value)\n",
    "                if lower_value in lower_valid_values:\n",
    "                    d[key] = valid_values[lower_valid_values.index(lower_value)]\n",
    "                    # print(d)\n",
    "                else:\n",
    "                    d[key] = 'na'\n",
    " \n",
    "    return d\n",
    "\n",
    "def convert_date_format(response, key):\n",
    "    if key in response:\n",
    "        value = response[key]\n",
    "        if not isinstance(value, str):\n",
    "            response[key] = 'na'\n",
    "            return response\n",
    "        try:\n",
    "            parsed_date = parser.parse(value)\n",
    "            formatted_date = parsed_date.strftime('%m/%d/%Y')\n",
    "            response[key] = formatted_date\n",
    "        except ValueError:\n",
    "            response[key] = 'na'\n",
    "    return response\n",
    "\n",
    "def convert_pipe_to_space(dictionary):\n",
    "    # Iterate over each key-value pair in the dictionary\n",
    "    for key, value in dictionary.items():\n",
    "        # If the value is a string and contains '|'\n",
    "        if isinstance(value, str) and '|' in value:\n",
    "            # Replace '|' with space\n",
    "            dictionary[key] = value.replace('|', ' ')\n",
    "    return dictionary\n",
    "\n",
    "def process_clause_section(response):\n",
    "    for key in response.keys():\n",
    "        if 'clause' in key.lower():\n",
    "            response[key] = ''.join(ch if ch.isalnum() or ch in ['.', '-'] else ' ' for ch in response[key])\n",
    "    return response\n",
    "\n",
    "def remove_no_na_response(response):\n",
    "    # Get the first key of the dictionary\n",
    "    first_key = list(response.keys())[0]\n",
    "\n",
    "    if response[first_key].lower() == 'no':\n",
    "        for key in response.keys():\n",
    "            if 'clause' in key.lower():\n",
    "                if response[key].lower() == 'na':\n",
    "                    for key in response.keys():\n",
    "                        response[key] = 'na'\n",
    "                        \n",
    "    return response\n",
    "\n",
    "def get_details_Payment_terms(chunks):\n",
    "     \n",
    "    l = []\n",
    "    for text in chunks:\n",
    "        keywords = ['Payment terms','Payment Terms in days','Payment Terms_Clause Reference']\n",
    "        \n",
    "\n",
    "        output_format = \"\"\"\n",
    "            {\"<Payment terms>\":\"<Identified answer>\",\n",
    "            \"<Payment Terms in days>\":\"<Identified answer>\",           \n",
    "            \"<Payment Terms_Clause Reference>\":\"<Identified answer>\"}\"\"\"\n",
    "                \n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are a master service agreement (MSA) details provider AI assistance.\\n\n",
    "        Your task is you will be provided a list of keywords {keywords} you need to extract the relevant answer from the MSA Agreement.\\n\n",
    "        You will be provided a top keyword matching pages from the MSA Agreement delimited by triple backticks.\\n\n",
    "        Do not provide extra explantion to your answer. If you are unable to extract the answer, then leave it as 'na'. Do strictly follow the below instruction strictly.\\n\n",
    "        \n",
    "        Instruction:\\n\n",
    " \n",
    "        Step1: Understand the contract and locate the 'Payment terms'. Based on the context, set 'payment terms' to either 'Numeric' or if statement of work is mentioned, then set it to 'As specified by SOW'. Other than these two options, don't provide response to payment terms..\\n\n",
    "        Step2: If 'Payment terms' is 'Numeric' then extract the payment terms in days value and set it to 'Payment Terms in days' elif 'Payment terms' is not 'numeric' then set 'Payment Terms in days' to 'na'.\\n \n",
    "        Step3: If the value of 'payment terms' is either **Numeric** or **As specified by SOW**, you extract the corresponding clause/subclause/Appendix/Execution/Annex/Exhibit and assign it to 'Payment Terms_Clause Reference' where the payment terms are mentioned. If you unable to extract 'Payment Terms_Clause Reference' then leave it as 'na'.\n",
    "        'Payment terms_Clause Reference' this value should be in the form of: 'Sec <section number>'.\n",
    "        Step4: Display the response strictly in a JSON format:{output_format}. \n",
    "\n",
    "        MSA Agreement : ```{text}```\n",
    "        keywords: {keywords}\n",
    "        \"\"\"\n",
    "        for _ in range(1):\n",
    "            \n",
    "            try:\n",
    "\n",
    "                response = get_completion(prompt)\n",
    "                response = ast.literal_eval(response)\n",
    "                print('response------------------------------', response)\n",
    "\n",
    "                #changing the key name in response\n",
    "                new_keys= ['PaymentTerms','PaymentTermsInDays','Payment Terms_Clause Reference']\n",
    "                response = map_keys_by_order(response,new_keys)\n",
    "                response = convert_pipe_to_space(response)\n",
    "                response = process_clause_section(response)\n",
    "                #validation\n",
    "                valid_values = ['Numeric', 'As specified in SOW', 'na']\n",
    "                response = process_dict(response, 'PaymentTerms', valid_values)\n",
    "\n",
    "                response = process_payment_terms(response)  #checking numeric - days in integer \n",
    "                print('response checking numeric int:',response)\n",
    "\n",
    "                response = process_payment_clause(response)\n",
    "                print('response checking clause :',response)\n",
    "                \n",
    "                response = remove_no_na_response(response)\n",
    "                print('removing No and na response-------------',response)\n",
    "\n",
    "                #If all values are na then not considering\n",
    "                if all(value.lower() != 'na' for value in response.values()):\n",
    "                    print(\"Not all values are 'na'. Proceeding to the next text.\")\n",
    "                    # continue\n",
    "\n",
    "                    response = format_response(text,response)\n",
    "                    print('formated response------------------------------',response)\n",
    "\n",
    "                    l.append(response)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if '429' in str(e):  # If error is 429\n",
    "                    print(\"429 error encountered. Waiting for 4 seconds before retrying.\")\n",
    "                    time.sleep(4)  # Wait for 4 seconds before retrying\n",
    "                else:\n",
    "                    print(f\"Error processing text: {e}\")\n",
    "                    break  # If error is not 429, break the retry loop and continue with the next text\n",
    "    return l\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3def755-aebe-4cb6-948d-580d4693dd66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keyvault' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 301\u001b[0m\n\u001b[0;32m    298\u001b[0m account_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazstorageaccnpfinoper\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    299\u001b[0m container_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m757\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 301\u001b[0m identity_client_id \u001b[38;5;241m=\u001b[39m \u001b[43mkeyvault\u001b[49m\u001b[38;5;241m.\u001b[39mget_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclientid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m### Authenticate using MSI\u001b[39;00m\n\u001b[0;32m    304\u001b[0m credential \u001b[38;5;241m=\u001b[39m ManagedIdentityCredential(client_id\u001b[38;5;241m=\u001b[39midentity_client_id)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keyvault' is not defined"
     ]
    }
   ],
   "source": [
    "def detect_languages(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        return 'Unable to detect language'\n",
    "    \n",
    "    if lang == 'en':\n",
    "        return 'english'\n",
    "    else:\n",
    "        return 'Unable to detect language'\n",
    "\n",
    "def read_pdf_and_detect_language(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    \n",
    "    language = detect_languages(text)\n",
    "    return language\n",
    "\n",
    "def process(filename):\n",
    "\n",
    "    def keywords_embeddings(keywords, filename):\n",
    "        print(\"inside keywords_embeddings\")\n",
    "        output_folder = \"keyword_embeddings/\"\n",
    "        os.makedirs(os.path.dirname(output_folder),exist_ok = True)\n",
    "        keywords_1 = keywords.replace(\" \",\"_\")\n",
    "        output_path = os.path.join(output_folder, keywords_1 + \"_\" + filename.replace(\".PDF\",\".pkl\"))\n",
    "        return output_path\n",
    "\n",
    "    keywords1 = 'payment terms'\n",
    "    output_path1 = keywords_embeddings(keywords1, filename)\n",
    "    keywords_embeddings_1 = get_embeddings(keywords1,output_path1)\n",
    "\n",
    "    return None\n",
    "\n",
    "def file_failure_status(filename,message):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    cols = ['PaymentTerms','PaymentTermsInDays']\n",
    "\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 'nan'\n",
    "\n",
    "    for col in df.columns:\n",
    "\n",
    "        # Filter out NaN values\n",
    "        df_cleaned = df.dropna()\n",
    "        \n",
    "        # Convert cleaned DataFrame to dictionary with index numbering keys\n",
    "        json_data = {}\n",
    "        for col in df_cleaned.columns:\n",
    "            json_data[col] = [{str(i): v for i, v in enumerate(df_cleaned[col])}]\n",
    "\n",
    "    # Prepare the result\n",
    "    result = {\"data\": [json_data]}\n",
    "\n",
    "    # Get the current time in IST\n",
    "\n",
    "    ist_timezone = pytz.timezone(\"Asia/Kolkata\")\n",
    "\n",
    "    current_time_ist = datetime.now(ist_timezone)\n",
    "\n",
    "    extractiontime = current_time_ist.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    print(\"Current IST time:\", current_time_ist.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    # Add a new key-value pair\n",
    "    result1 = {}\n",
    "\n",
    "    # Split the filename into name and extension\n",
    "    name, extension = os.path.splitext(filename)\n",
    "\n",
    "    # Replace the .pdf extension with .json\n",
    "    json_filename = name \n",
    "\n",
    "    result1[\"AIExtractionStatus\"] = \"Failure\"\n",
    "    \n",
    "    result1[\"Message\"] = message\n",
    "\n",
    "    result1[\"UniqueID\"]=json_filename\n",
    "\n",
    "    result1[\"AIExtractionDateTime\"]= extractiontime\n",
    "\n",
    "    result1.update(result)\n",
    "\n",
    "    print(json.dumps(result1, indent=4))\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(local_outputfolder, exist_ok=True)\n",
    "\n",
    "    # Full path to the file\n",
    "    output_file_path = os.path.join(local_outputfolder, json_filename + '.json')\n",
    "\n",
    "    # Write the JSON object to the file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(result1, f, indent=4)\n",
    "\n",
    "    print(f\"JSON has been written to {output_file_path}\")\n",
    "    return filename\n",
    "    \n",
    "    \n",
    "def extraction(filename):\n",
    "    print('**************inside extraction functions***************')\n",
    "    print('filename-------------',filename)\n",
    "    try:\n",
    "        #Reading the pdf text from output_folder2\n",
    "        output_folder2= \"pdf_text/\"\n",
    "        output_path2 = os.path.join(output_folder2,filename.replace(\".PDF\",\".txt\"))\n",
    "        print('Reading pdf text inside extraction')\n",
    "        pdf_text_1 = read_list_from_file(output_path2)\n",
    "        pdf_text = ast.literal_eval(pdf_text_1)\n",
    "\n",
    "        # Reading the summary text from output_folder3\n",
    "        output_folder3= \"summary_text/\"\n",
    "        output_path3 = os.path.join(output_folder3,filename.replace(\".PDF\",\".txt\"))\n",
    "        print('reading summary text inside extraction')\n",
    "        summary_text = read_list_from_file(output_path3)\n",
    "        summary_text1 = ast.literal_eval(summary_text)\n",
    "        summary = summary1(summary_text1)\n",
    "\n",
    "        def keywords_path(keywords,filename):\n",
    "            output_folder = \"keyword_embeddings/\"\n",
    "            keywords1 = keywords.replace(\" \",\"_\")\n",
    "            output_path = os.path.join(output_folder,keywords1 + \"_\" + filename.replace(\".PDF\",\".pkl\"))\n",
    "            return output_path\n",
    "\n",
    "        #loading pdf text embedding from new_embedding folder\n",
    "        output_folder= \"new_embedding/\"\n",
    "        output_path = os.path.join(output_folder,filename.replace(\".PDF\",\".pkl\"))\n",
    "        embeddings = load_embeddings_from_pickle(output_path)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            keywords2 = 'payment terms'\n",
    "            output_path2 = keywords_path(keywords2, filename)\n",
    "            keywords_embeddings2 = load_embeddings_from_pickle(output_path2)\n",
    "            similar_chunks2 = find_closely_related_chunks(keywords_embeddings2, embeddings, pdf_text)\n",
    "            top_3_similar_chunks2 = create_chunks(similar_chunks2,chunk_size=1)\n",
    "            print('length of chunk size: ',len(top_3_similar_chunks2))\n",
    "            output2 = get_details_Payment_terms(top_3_similar_chunks2)\n",
    "        except Exception as e:\n",
    "            output2 = []\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            \n",
    "        out = output2\n",
    "\n",
    "        combined_data = {}\n",
    "        for d in out:\n",
    "            for key, value in d.items():\n",
    "                if key in combined_data:\n",
    "                    combined_data[key].append(value)\n",
    "                else:\n",
    "                    combined_data[key] = [value]\n",
    "\n",
    "        if combined_data:\n",
    "            try:\n",
    "                # Create a DataFrame for each key-value pair and concatenate them\n",
    "                df = pd.concat({k: pd.DataFrame(v) for k, v in combined_data.items()}, axis=1)\n",
    "\n",
    "                # Drop the first level of the column names\n",
    "                df.columns = df.columns.droplevel(1)\n",
    "\n",
    "                df = df.rename(columns={'ForceMajeure': 'ForceMajeureClause', 'ForceMajeureLinkToOther': 'ForceMajeureLinkToOtherClauses'})\n",
    "\n",
    "                cols = ['PaymentTerms','PaymentTermsInDays']\n",
    "\n",
    "                for col in cols:\n",
    "                    if col not in df.columns:\n",
    "                        df[col] = 'nan'\n",
    "\n",
    "                for col in df.columns:\n",
    "                    df[col] = df[col].astype(str).str.replace(':', '|')\n",
    "\n",
    "                    # Filter out NaN values\n",
    "                    df_cleaned = df.dropna()\n",
    "\n",
    "\n",
    "                    # Convert cleaned DataFrame to dictionary with index numbering keys\n",
    "                    json_data = {}\n",
    "                    for col in df_cleaned.columns:\n",
    "                        json_data[col] = [{str(i): v for i, v in enumerate(df_cleaned[col])}]\n",
    "\n",
    "                # Prepare the result\n",
    "                result = {\"data\": [json_data]}\n",
    "\n",
    "                # Iterate over the dictionary\n",
    "                for item in result['data']:\n",
    "                    for sub_item in item.values():\n",
    "                        for dict_item in sub_item:\n",
    "                            for key in list(dict_item.keys()):  # Use list to create a copy of keys\n",
    "                                value = dict_item[key]\n",
    "                                if value.split('|')[0] in ['na', 'Not mentioned', 'Not defined', 'Not Mentioned', 'Not Defined']:\n",
    "                                    del dict_item[key]  # Remove the pair from the dictionary\n",
    "\n",
    "                def remove_nan_pairs_and_reset_index(obj):\n",
    "                    if isinstance(obj, dict):\n",
    "                        new_obj = {k: remove_nan_pairs_and_reset_index(v) for k, v in obj.items() if v != 'nan'}\n",
    "                        if all(k.isdigit() for k in new_obj.keys()):\n",
    "                            new_obj = {str(i): v for i, (k, v) in enumerate(new_obj.items())}\n",
    "                        return new_obj\n",
    "                    elif isinstance(obj, list):\n",
    "                        return [remove_nan_pairs_and_reset_index(item) for item in obj]\n",
    "                    else:\n",
    "                        return obj\n",
    "\n",
    "\n",
    "                # Remove 'nan' pairs\n",
    "                filtered_data = remove_nan_pairs_and_reset_index(result)\n",
    "\n",
    "                # Get the current time in IST\n",
    "\n",
    "                ist_timezone = pytz.timezone(\"Asia/Kolkata\")\n",
    "\n",
    "                current_time_ist = datetime.now(ist_timezone)\n",
    "\n",
    "                extractiontime = current_time_ist.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                print(\"Current IST time:\", current_time_ist.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "                # Add a new key-value pair\n",
    "                result1 = {}\n",
    "\n",
    "                # Split the filename into name and extension\n",
    "                name, extension = os.path.splitext(filename)\n",
    "\n",
    "                # Replace the .pdf extension with .json\n",
    "                # json_filename = name + '.json'\n",
    "                json_filename = name \n",
    "\n",
    "                result1[\"AIExtractionStatus\"] = \"Success\"\n",
    "\n",
    "                result1[\"UniqueID\"]=json_filename\n",
    "\n",
    "                result1[\"AIExtractionDateTime\"]= extractiontime\n",
    "\n",
    "                result1.update(filtered_data)\n",
    "\n",
    "                print(json.dumps(result1, indent=4))\n",
    "\n",
    "                # Create the folder if it doesn't exist\n",
    "                os.makedirs(local_outputfolder, exist_ok=True)\n",
    "\n",
    "                # Full path to the file\n",
    "                output_file_path = os.path.join(local_outputfolder, filename.replace(\".PDF\",\".json\"))\n",
    "\n",
    "                # Write the JSON object to the file\n",
    "                with open(output_file_path, 'w') as f:\n",
    "                    json.dump(result1, f, indent=4)\n",
    "\n",
    "                print(f\"JSON has been written to {output_file_path}\")\n",
    "\n",
    "                return filename\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Error occurred while creating DataFrame: {e}\")\n",
    "                message = \"Error occurred while creating DataFrame\"\n",
    "                result1 = file_failure_status(filename,message)\n",
    "            \n",
    "        else:\n",
    "            print(\"GPT is not able to extract. All the fields are empty\")\n",
    "            message = \"GPT is not able to extract. All the fields are empty\"\n",
    "            result1 = file_failure_status(filename,message)\n",
    "            return filename\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Expection occured while reading the files: {e}\")\n",
    "        message = \"Expection occured\"\n",
    "        result1 = file_failure_status(filename,message)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def list_files(directory):\n",
    "    # print(os.listdir(directory))\n",
    "    return [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "\n",
    "def upload_file_to_blob(blob_service_client, file_path, container_name, blob_name):\n",
    "    try:\n",
    "        # Create a blob client using the local file name as the name for the blob\n",
    "        blob_client = blob_service_client.get_blob_client(container_name, blob_name)\n",
    "        # Upload the created file\n",
    "        with open(file_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, content_type=\"application/json\",overwrite=True)\n",
    "    except Exception as e:\n",
    "        print(\"AzureError: \", str(e))\n",
    "\n",
    "def upload_files_to_blob_folder(blob_service_client, local_outputfolder, container_name, blob_outputfolder):\n",
    "    for filename in os.listdir(local_outputfolder):#folder_path\n",
    "        local_path = os.path.join(local_outputfolder, filename)\n",
    "        blob_name = os.path.join(blob_outputfolder, filename)\n",
    "        print(f\"Uploading {local_path} to {blob_name}\")\n",
    "        upload_file_to_blob(blob_service_client, local_path, container_name, blob_name)\n",
    "\n",
    "account_name = 'azstorageaccnpfinoper'\n",
    "container_name = '757'\n",
    "\n",
    "identity_client_id = keyvault.get_secret(\"clientid\")\n",
    "\n",
    "### Authenticate using MSI\n",
    "credential = ManagedIdentityCredential(client_id=identity_client_id)\n",
    "print(\"credential\", credential)\n",
    "\n",
    "global local_inputfolder, local_pdf_text, local_summary_text, local_new_embedding, localinput_foldername, blob_outputfolder, blob_inputfolder, blob_preprocessedfolder, blob_processedfolder\n",
    "\n",
    "local_inputfolder = \"Input\"\n",
    "local_outputfolder = 'Output'\n",
    "\n",
    "# local_pdf_text = \"MSA_test/pdf_text\"\n",
    "# local_summary_text = \"MSA_test/summary_text\"\n",
    "# local_new_embedding = \"MSA_test/new_embedding\"\n",
    "localinput_foldername = \"Input/MSA_test/Input\"\n",
    "\n",
    "blob_outputfolder = 'MSA_test/OUTPUT'\n",
    "\n",
    "blob_inputfolder = 'MSA_test/Input'\n",
    "blob_preprocessedfolder = 'MSA_test/Pre-Processed'\n",
    "blob_processedfolder = 'MSA_test/Processed'\n",
    "\n",
    "### Authenticate using MSI\n",
    "# credential = ManagedIdentityCredential(client_id=identity_client_id)\n",
    "# print(\"credential\", credential)\n",
    "# Initialize the BlobServiceClient\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=credential)\n",
    "# Get the ContainerClient\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "# List all the blobs in the specific folder\n",
    "blob_list = container_client.list_blobs(name_starts_with=blob_inputfolder)\n",
    "\n",
    "## Checking Whether Output, pre-processed and Processed folders are avialable in Blob or not.\n",
    "## If not then first create folders\n",
    "blob_client_output = container_client.get_blob_client(blob=blob_outputfolder)\n",
    "blob_client_processed = container_client.get_blob_client(blob=blob_processedfolder)\n",
    "blob_client_preprocessed = container_client.get_blob_client(blob = blob_preprocessedfolder)\n",
    "        \n",
    "if not blob_client_output.exists():\n",
    "    print(f\"The folder '{blob_client_output}' does not exists.\")\n",
    "    blob_client_output.upload_blob(b\"\", overwrite=True)\n",
    "    \n",
    "else:\n",
    "    print(f\"The folder '{blob_client_output}' exist.\")\n",
    "\n",
    "    \n",
    "if not blob_client_processed.exists():\n",
    "    print(f\"The folder '{blob_client_processed}' does not exists.\")\n",
    "    blob_client_processed.upload_blob(b\"\", overwrite=True)\n",
    "    \n",
    "else:\n",
    "    print(f\"The folder '{blob_client_processed}' exist.\")\n",
    "\n",
    "if not blob_client_preprocessed.exists():\n",
    "    print(f\"The folder '{blob_client_preprocessed}' does not exists.\")\n",
    "    blob_client_preprocessed.upload_blob(b\"\", overwrite=True)\n",
    "    \n",
    "else:\n",
    "    print(f\"The folder '{blob_client_preprocessed}' exist.\")\n",
    "\n",
    "# Check any files are available in Input Folder\n",
    "if len(list(container_client.list_blobs(name_starts_with=blob_inputfolder))) != 0:\n",
    "    print(\"Input Folder in not Empty. Processing files\")\n",
    "    ## Download all the blobs to your local\n",
    "    for blob in blob_list:\n",
    "        st_blob_down = time.time()\n",
    "        print(f\"\\nDownloading the blob '{blob.name}' from the container to the local folder...'{local_inputfolder}/{blob.name}' \")\n",
    "        os.makedirs(os.path.dirname(f\"{local_inputfolder}/{blob.name}\"), exist_ok=True)\n",
    "        with open(f\"{local_inputfolder}/{blob.name}\", \"wb\") as data:\n",
    "            blob_client = blob_service_client.get_blob_client(container_name, blob.name)\n",
    "            download_stream = blob_client.download_blob()\n",
    "            data.write(download_stream.readall())    \n",
    "            \n",
    "\n",
    "        ### List all the files present in Input folder \n",
    "    files = list_files(localinput_foldername)\n",
    "    print(\"List of files::\", files)\n",
    "    end_blob_down = time.time()\n",
    "    print('Time taken for downloading the files from blob',end_blob_down-st_blob_down)\n",
    "          \n",
    "    ###Moving all files from Input folder to Pre-Processed folder\n",
    "    for blob in files:\n",
    "        source_blob_path = f\"{blob_inputfolder}/{blob}\"\n",
    "        print('source_blob_path', source_blob_path)\n",
    "        destination_blob_path = f\"{blob_preprocessedfolder}/{blob}\"\n",
    "        print('destination_blob_path', destination_blob_path)    \n",
    "        source_blob_client = container_client.get_blob_client(source_blob_path)\n",
    "        print('source_blob_client', source_blob_client)\n",
    "        if source_blob_client.exists():\n",
    "            destination_blob_client = container_client.get_blob_client(destination_blob_path)\n",
    "            print('destination_blob_client', destination_blob_client)\n",
    "            destination_blob_client.start_copy_from_url(source_blob_client.url)\n",
    "            source_blob_client.delete_blob()\n",
    "        else:\n",
    "            print(f\"Blob '{blob}' not found in the source container.\")\n",
    "            \n",
    "    def main(filename):\n",
    "        print('***********inside main function*************')\n",
    "        result = []\n",
    "        print(\"inside main func\")\n",
    "        for files in filename:\n",
    "            print('files',files)\n",
    "            start_ex = time.time()\n",
    "            output = extraction(files)\n",
    "            end_ex = time.time()\n",
    "            print('Time taken for extraction',end_ex-start_ex)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    def is_english(text):\n",
    "        try:\n",
    "            language = detect(text)\n",
    "        except:\n",
    "            return \"Unable to detect language\"\n",
    "\n",
    "        if language == 'en':\n",
    "            return \"English\"\n",
    "        else:\n",
    "            return \"Unable to detect language\"\n",
    "\n",
    "        \n",
    "    def check_valid_files(files):\n",
    "        result = []\n",
    "        for file_path in files:\n",
    "            print('file_path-----------',file_path)\n",
    "            if file_path.endswith('.PDF'):\n",
    "                result.append(file_path)\n",
    "            else:\n",
    "                message = 'File is not Pdf'\n",
    "                output = file_failure_status(file_path,message)    \n",
    "        return result\n",
    "    \n",
    "    def file_validation(file):\n",
    "        print('******Inside file validation function************')\n",
    "        result1 = []\n",
    "        for filename in file:\n",
    "            endpoint = \"https://ctsazformrecpdfinops.cognitiveservices.azure.com/\"\n",
    "            #Reading pdf file from localinput_foldername\n",
    "            file_path = os.path.join(localinput_foldername,filename) \n",
    "            start_re = time.time()\n",
    "            output_json = reading_pdf(file_path, endpoint, credential)\n",
    "            output_image_text = extract_text_from_pdf(file_path, endpoint, credential)\n",
    "            \n",
    "            output_json_dict = {list(d.keys())[0]: list(d.values())[0] for d in output_json}\n",
    "            output_image_text_dict = {list(d.keys())[0]: list(d.values())[0] for d in output_image_text}\n",
    "            # Find keys in output_json1 not present in output_json\n",
    "            new_keys = {k: v for k, v in output_image_text_dict.items() if k not in output_json_dict}\n",
    "            # Convert the dictionary to a list of dictionaries\n",
    "            new_keys_list = [{k: v} for k, v in new_keys.items()]\n",
    "    \n",
    "            output_json.extend(new_keys_list)\n",
    "            end_re = time.time()\n",
    "            print('Time taken for reading pdf',end_re-start_re)\n",
    "            pdf_text = output_json\n",
    "            combined_text = \" \".join([value for dictionary in pdf_text for value in dictionary.values()])\n",
    "            # print('combined_text--------------------',combined_text)\n",
    "            language = is_english(combined_text)\n",
    "            first_page_text = list(pdf_text[0].values())[0]\n",
    "\n",
    "            if \"Get Adobe Reader\" in first_page_text:\n",
    "                print(\"PDF is not opened\",filename)\n",
    "                message = \"PDF is not Opened\"\n",
    "                result = file_failure_status(filename,message) \n",
    "\n",
    "            elif \"Unable to detect language\" in language:\n",
    "                print(\"PDF is not in english\",filename)\n",
    "                message = \"PDF is not in English\"\n",
    "                result = file_failure_status(filename,message)  \n",
    "\n",
    "            else:\n",
    "                print('PDF can be processed',filename)\n",
    "                #Saving the extracted text to output_folder2\n",
    "                output_folder2= \"pdf_text/\"\n",
    "                os.makedirs(os.path.dirname(output_folder2),exist_ok = True)\n",
    "                output_path2 = os.path.join(output_folder2,filename.replace(\".PDF\",\".txt\"))\n",
    "                plain_text = save_list_to_file(pdf_text, output_path2)\n",
    "\n",
    "                output_summary = pdf_text[:10]\n",
    "\n",
    "                #Saving the extracted text to output_folder2\n",
    "                output_folder3= \"summary_text/\"\n",
    "                os.makedirs(os.path.dirname(output_folder3),exist_ok = True)\n",
    "                output_path3 = os.path.join(output_folder3,filename.replace(\".PDF\",\".txt\"))\n",
    "                summary_text = save_list_to_file(output_summary, output_path3)\n",
    "\n",
    "                #Embedding the pdf text and saving in output_folder\n",
    "                output_folder= \"new_embedding/\"\n",
    "                os.makedirs(os.path.dirname(output_folder),exist_ok = True)\n",
    "                output_path_emb = os.path.join(output_folder,filename.replace(\".PDF\",\".pkl\"))\n",
    "                embeddings_1= get_embeddings(pdf_text, output_path_emb)\n",
    "                result1.append(filename)\n",
    "        return result1\n",
    "\n",
    "   \n",
    "    # def final(path):\n",
    "    st = time.time()\n",
    "    print('***********print inside final function****')\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "    all_files = list_files(localinput_foldername)\n",
    "    print(\"(1)all_files--------------\",all_files)\n",
    "    all_files1 = check_valid_files(all_files) # cheking the file type\n",
    "    print('all valid files--------',all_files1) \n",
    "    files = file_validation(all_files1) \n",
    "    print(\"(2)files--------------\",files)\n",
    "    data_chunks = np.array_split(files, num_cores)\n",
    "    print('data_chunks: ',data_chunks)\n",
    "    print(\"(2)embeddings----------------------\")\n",
    "    se = time.time()\n",
    "    embeddings = [process(file) for file in files]## creation of embedding for all the files\n",
    "    ee = time.time()\n",
    "    print(\"(3)embeddings completed--------------\")\n",
    "    sm = time.time()\n",
    "    pool = multiprocessing.Pool(processes=num_cores)\n",
    "    results = pool.map(main, data_chunks)\n",
    "    print('(4)result step completed--------------------')\n",
    "    # finaloutput = np.concatenate(results)\n",
    "    em = time.time()\n",
    "    # print(\"length of Final Output:::\", len(finaloutput))\n",
    "    # print(\"Final Output:::\",finaloutput )\n",
    "    print(\"Time taken for reading and embeddings pdf \",ee-se)\n",
    "    print(\"Time taken for extraction \",em-sm)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    et = time.time()\n",
    "    print('Total time taken ',et-st)\n",
    "\n",
    "    os.makedirs(local_outputfolder, exist_ok=True)\n",
    "    ###Uploding json back to Output folder of Blob\n",
    "    if blob_service_client is not None:\n",
    "        print(\"*********inside uploading**********\")\n",
    "        upload_files_to_blob_folder(blob_service_client, local_outputfolder, container_name, blob_outputfolder)\n",
    "        ### Move the processed file to processed folder of blob\n",
    "        for blob in all_files:\n",
    "            print(\"blob------------\",blob)\n",
    "            source_blob_path = f\"{blob_preprocessedfolder}/{blob}\"\n",
    "            print('source_blob_path', source_blob_path)\n",
    "            destination_blob_path = f\"{blob_processedfolder}/{blob}\"\n",
    "            print('destination_blob_path', destination_blob_path)    \n",
    "            source_blob_client = container_client.get_blob_client(source_blob_path)\n",
    "            print('source_blob_client', source_blob_client)\n",
    "            if source_blob_client.exists():\n",
    "                destination_blob_client = container_client.get_blob_client(destination_blob_path)\n",
    "                print('destination_blob_client', destination_blob_client)\n",
    "                destination_blob_client.start_copy_from_url(source_blob_client.url)\n",
    "                source_blob_client.delete_blob()\n",
    "            else:\n",
    "                print(f\"Blob '{blob}' not found in the source container.\")\n",
    "    else:\n",
    "        print(\"Error in logging into blob service client.\")\n",
    "else:\n",
    "    print('Input folder is empty')\n",
    "    \n",
    "    \n",
    "end_time = time.time()\n",
    "print('Time taken for running the whole script',end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bee204-6494-44e8-875c-ba31126e54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"MLNPFINOPSCOMPCLUSTER01\"\n",
    "\n",
    "try:\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    print('Compute Cluster is not available')\n",
    "    # If not, create it. The below statement will not run in non prod or prod due to lack of permission, only for lower env\n",
    "    # compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D4_V3', max_nodes=3)\n",
    "    # pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "pipeline_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f14b5e-9956-447e-b95a-c8ec9502f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "my_env = Environment.from_conda_specification(name = \"test_lang_MSA\",\n",
    "                                           file_path ='./MSAsdk.yml')\n",
    "\n",
    "my_env.python.user_managed_dependencies = False                                    # Let Azure ML manage dependencies\n",
    "my_env.docker.enabled = True                                                       # Use a docker container\n",
    "\n",
    "# Register the environment (just in case previous lab wasn't completed)\n",
    "my_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'test_lang_MSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11c331-d4f9-4014-bd06-5b9af5bd1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d441089-a37f-42fd-bb1c-b7c5013434bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core import Datastore, Dataset\n",
    "from datetime import datetime\n",
    "\n",
    "train_step_pd_1 = PythonScriptStep(name = \"test_pipeline_MSATraining\",\n",
    "                                   script_name='Training.py',\n",
    "                                   compute_target=pipeline_cluster,\n",
    "                                   source_directory=experiment_folder,\n",
    "                                   runconfig=pipeline_run_config,\n",
    "                                   allow_reuse=False)\n",
    "print(\"python_script_step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f3437-2046-4ea2-83dd-56dc15c0d2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline_steps = [train_step_pd_1]\n",
    "\n",
    "pipeline = Pipeline(workspace = ws, steps=pipeline_steps)\n",
    "\n",
    "print(\"Pipeline is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace0a1c-9a47-4cfb-8e2d-9e6f2f4c3409",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name=\"test_pipeline_MSATraining\", \n",
    "                                        description=\"NONProd Pipeline for MSA Extraction\", \n",
    "                                        continue_on_step_failure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe83f0-fcc0-4a4e-a4d9-803f942bda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "\n",
    "pipeline_id = published_pipeline.id # use your published pipeline id\n",
    "published_pipeline = PublishedPipeline.get(ws, pipeline_id)\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103de7b-deb0-408c-8efe-351864d57835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core import Datastore, Dataset\n",
    "# from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule, TimeZone\n",
    "\n",
    "# # Submit the Pipeline every 5 minutes\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Minute\", interval=60)\n",
    "# experiment_name = \"MSA_Prod\"\n",
    "\n",
    "# reactive_schedule = Schedule.create(ws, name=\"MSA_Prod_Pipeline\", \n",
    "#                                     description=\"Prod Pipeline for MSA Extraction\",\n",
    "#                                     pipeline_id=pipeline_id, experiment_name=experiment_name, \n",
    "#                                     # datastore=datastore,\n",
    "#                                    recurrence =recurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b4dd0-0f2f-46a6-b31c-1043c3b67456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
